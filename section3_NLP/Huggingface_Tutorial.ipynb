{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\admin\\AppData\\Local\\Temp\\ipykernel_18476\\1795685252.py:1: DeprecationWarning: \n",
      "Pyarrow will become a required dependency of pandas in the next major release of pandas (pandas 3.0),\n",
      "(to allow more performant data types, such as the Arrow string type, and better interoperability with other libraries)\n",
      "but was not found to be installed on your system.\n",
      "If this would cause problems for you,\n",
      "please provide us feedback at https://github.com/pandas-dev/pandas/issues/54466\n",
      "        \n",
      "  import pandas as pd\n",
      "c:\\Users\\admin\\Desktop\\Pytorch_study\\.venv\\Lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from transformers import AutoModel, AutoTokenizer, BertTokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "config.json: 100%|██████████| 570/570 [00:00<?, ?B/s] \n",
      "c:\\Users\\admin\\Desktop\\Pytorch_study\\.venv\\Lib\\site-packages\\huggingface_hub\\file_download.py:149: UserWarning: `huggingface_hub` cache-system uses symlinks by default to efficiently store duplicated files but your machine does not support them in C:\\Users\\admin\\.cache\\huggingface\\hub\\models--bert-base-cased. Caching files will still work but in a degraded version that might require more space on your disk. This warning can be disabled by setting the `HF_HUB_DISABLE_SYMLINKS_WARNING` environment variable. For more details, see https://huggingface.co/docs/huggingface_hub/how-to-cache#limitations.\n",
      "To support symlinks on Windows, you either need to activate Developer Mode or to run Python as an administrator. In order to see activate developer mode, see this article: https://docs.microsoft.com/en-us/windows/apps/get-started/enable-your-device-for-development\n",
      "  warnings.warn(message)\n",
      "model.safetensors: 100%|██████████| 436M/436M [00:07<00:00, 61.5MB/s] \n"
     ]
    }
   ],
   "source": [
    "# Store the model \n",
    "BERT_MODEL_NAME = 'bert-base-cased'\n",
    "\n",
    "# we need to create the model\n",
    "bert_model = AutoModel.from_pretrained(BERT_MODEL_NAME)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tokenizer 실습\n",
    "- 모델을 사용하려면, 해당 모델이 학습한 Tokenizer를 불러와야 한다.\n",
    "- 다행히, AutoModel 클래스를 사용하면, 우리가 불러온 모델에 대한 Tokenizer도 손쉽게 불러올 수 있다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "tokenizer_config.json: 100%|██████████| 29.0/29.0 [00:00<?, ?B/s]\n",
      "vocab.txt: 100%|██████████| 213k/213k [00:00<00:00, 578kB/s]\n",
      "tokenizer.json: 100%|██████████| 436k/436k [00:00<00:00, 1.18MB/s]\n"
     ]
    }
   ],
   "source": [
    "# need to create the tokenizer\n",
    "# 단어사전\n",
    "bert_tokenizer = AutoTokenizer.from_pretrained(BERT_MODEL_NAME)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "28996\n"
     ]
    }
   ],
   "source": [
    "print(bert_tokenizer.vocab_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "##hr\n",
      "AJ\n",
      "##ony\n",
      "deprived\n",
      "held\n",
      "Coventry\n",
      "Danube\n",
      "honorable\n",
      "issued\n",
      "carriage\n",
      "their\n",
      "built\n"
     ]
    }
   ],
   "source": [
    "for i, key in enumerate(bert_tokenizer.get_vocab()):\n",
    "    print(key)\n",
    "    if i>10:\n",
    "        break"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "우리가 불러온 BERT(\"bert-base-cased\") 모델은 BertTokenizerFast class로 되어 있다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'transformers.models.bert.tokenization_bert_fast.BertTokenizerFast'>\n"
     ]
    }
   ],
   "source": [
    "print(type(bert_tokenizer))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "sample_1 = 'welcome to the natural language class'\n",
    "sample_2 = 'welcometothenaturallanguageclass'\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "input_ids:\n",
      "\ttensor([[ 101, 7236, 1106, 1103, 2379, 1846, 1705,  102]])\n",
      "token_type_ids:\n",
      "\ttensor([[0, 0, 0, 0, 0, 0, 0, 0]])\n",
      "attention_mask:\n",
      "\ttensor([[1, 1, 1, 1, 1, 1, 1, 1]])\n"
     ]
    }
   ],
   "source": [
    "tokenized_input_text = bert_tokenizer(sample_1, return_tensors='pt')\n",
    "for key, value in tokenized_input_text.items():\n",
    "    print(\"{}:\\n\\t{}\".format(key, value))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "input_ids:\n",
      "\ttensor([[ 101, 7236, 1106, 1103, 2379, 1846, 1705,  102]])\n",
      "token_type_ids:\n",
      "\ttensor([[0, 0, 0, 0, 0, 0, 0, 0]])\n",
      "attention_mask:\n",
      "\ttensor([[1, 1, 1, 1, 1, 1, 1, 1]])\n"
     ]
    }
   ],
   "source": [
    "tokenized_input_text_merged = bert_tokenizer(sample_2, return_tensors='pt')\n",
    "for key, value in tokenized_input_text.items():\n",
    "    print(\"{}:\\n\\t{}\".format(key, value))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[ 101, 7236, 1106, 1103, 2379, 1846, 1705,  102]])\n",
      "tensor([[ 101, 7236, 1106, 1103, 2379, 1846, 1705,  102]])\n",
      "tensor([[0, 0, 0, 0, 0, 0, 0, 0]])\n",
      "tensor([[0, 0, 0, 0, 0, 0, 0, 0]])\n",
      "tensor([[1, 1, 1, 1, 1, 1, 1, 1]])\n",
      "tensor([[1, 1, 1, 1, 1, 1, 1, 1]])\n"
     ]
    }
   ],
   "source": [
    "# input text를 tokenizing한 후 vocab의 id\n",
    "print(tokenized_input_text['input_ids'])\n",
    "print(tokenized_input_text.input_ids)\n",
    "\n",
    "# segment id (sentA or sentB)\n",
    "print(tokenized_input_text['token_type_ids'])\n",
    "print(tokenized_input_text.token_type_ids)\n",
    "\n",
    "# special token (pad, cls, sep) or not\n",
    "print(tokenized_input_text['attention_mask'])\n",
    "print(tokenized_input_text.attention_mask)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['welcome', 'to', 'the', 'natural', 'language', 'class']\n",
      "[101, 7236, 1106, 1103, 2379, 1846, 1705, 102]\n",
      "[CLS] welcome to the natural language class [SEP]\n"
     ]
    }
   ],
   "source": [
    "tokenized_text = bert_tokenizer.tokenize(sample_1)\n",
    "print(tokenized_text)\n",
    "\n",
    "input_ids = bert_tokenizer.encode(sample_1)\n",
    "print(input_ids)\n",
    "\n",
    "decoded_ids = bert_tokenizer.decode(input_ids)\n",
    "print(decoded_ids)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['welcome', 'to', 'the', 'natural', 'language', 'class']\n",
      "[7236, 1106, 1103, 2379, 1846, 1705]\n",
      "welcome to the natural language class\n"
     ]
    }
   ],
   "source": [
    "tokenized_text = bert_tokenizer.tokenize(sample_1, add_special_tokens=False)\n",
    "print(tokenized_text)\n",
    "\n",
    "input_ids = bert_tokenizer.encode(sample_1, add_special_tokens=False)\n",
    "print(input_ids)\n",
    "\n",
    "decoded_ids = bert_tokenizer.decode(input_ids)\n",
    "print(decoded_ids)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['welcome', 'to', 'the', 'natural', 'language']\n"
     ]
    }
   ],
   "source": [
    "tokenized_text = bert_tokenizer.tokenize(\n",
    "    sample_1,\n",
    "    add_special_tokens=False,\n",
    "    max_length=5,\n",
    "    truncation=True\n",
    ")\n",
    "print(tokenized_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[7236, 1106, 1103, 2379, 1846]\n",
      "welcome to the natural language\n"
     ]
    }
   ],
   "source": [
    "input_ids = bert_tokenizer.encode(\n",
    "    sample_1,\n",
    "    add_special_tokens = False,\n",
    "    max_length=5,\n",
    "    truncation=True\n",
    ")\n",
    "print(input_ids)\n",
    "\n",
    "decoded_ids = bert_tokenizer.decode(input_ids)\n",
    "print(decoded_ids)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[PAD]\n",
      "0\n"
     ]
    }
   ],
   "source": [
    "print(bert_tokenizer.pad_token)\n",
    "print(bert_tokenizer.pad_token_id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['welcome', 'to', 'the', 'natural', 'language', 'class', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]']\n",
      "[7236, 1106, 1103, 2379, 1846, 1705, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n"
     ]
    }
   ],
   "source": [
    "tokenized_text = bert_tokenizer.tokenize(\n",
    "    sample_1,\n",
    "    add_special_tokens = False,\n",
    "    max_length = 20,\n",
    "    padding = 'max_length'\n",
    ")\n",
    "\n",
    "print(tokenized_text)\n",
    "\n",
    "input_ids = bert_tokenizer.encode(\n",
    "    sample_1,\n",
    "    add_special_tokens=False,\n",
    "    max_length= 20,\n",
    "    padding='max_length'\n",
    ")\n",
    "\n",
    "print(input_ids)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 실습1. 한국어 Tokenizer 실습"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Huggingface에서 사용할 수 있는 BERT(\"bert-base-cased\") 모델은 영어 데이터로만 학습을 한 모델.\n",
    "따라서, 한국어 문장을 토큰화하려고 하면, [UNK] 토큰으로 변환된다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "config.json: 100%|██████████| 625/625 [00:00<00:00, 625kB/s]\n",
      "c:\\Users\\admin\\Desktop\\Pytorch_study\\.venv\\Lib\\site-packages\\huggingface_hub\\file_download.py:149: UserWarning: `huggingface_hub` cache-system uses symlinks by default to efficiently store duplicated files but your machine does not support them in C:\\Users\\admin\\.cache\\huggingface\\hub\\models--bert-base-multilingual-cased. Caching files will still work but in a degraded version that might require more space on your disk. This warning can be disabled by setting the `HF_HUB_DISABLE_SYMLINKS_WARNING` environment variable. For more details, see https://huggingface.co/docs/huggingface_hub/how-to-cache#limitations.\n",
      "To support symlinks on Windows, you either need to activate Developer Mode or to run Python as an administrator. In order to see activate developer mode, see this article: https://docs.microsoft.com/en-us/windows/apps/get-started/enable-your-device-for-development\n",
      "  warnings.warn(message)\n",
      "model.safetensors: 100%|██████████| 714M/714M [00:11<00:00, 61.6MB/s] \n",
      "tokenizer_config.json: 100%|██████████| 29.0/29.0 [00:00<00:00, 29.0kB/s]\n",
      "vocab.txt: 100%|██████████| 996k/996k [00:00<00:00, 1.78MB/s]\n",
      "tokenizer.json: 100%|██████████| 1.96M/1.96M [00:00<00:00, 2.09MB/s]\n"
     ]
    }
   ],
   "source": [
    "MULTI_BERT_MODEL_NAME = 'bert-base-multilingual-cased'\n",
    "\n",
    "multi_bert_model = AutoModel.from_pretrained(MULTI_BERT_MODEL_NAME)\n",
    "multi_bert_tokenizer = AutoTokenizer.from_pretrained(MULTI_BERT_MODEL_NAME)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "119547\n"
     ]
    }
   ],
   "source": [
    "print(multi_bert_tokenizer.vocab_size)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tokenizer에 새로운 Token 추가"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "unk_text = '한꾺인 뜰만 알아볼 쑤 있꼐 짝썽하껬씁니따'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['[UNK]', '[UNK]', '알', '##아', '##볼', '쑤', '있', '##께', '[UNK]']\n",
      "[100, 100, 9524, 16985, 101450, 9510, 9647, 118683, 100]\n",
      "[UNK] [UNK] 알아볼 쑤 있께 [UNK]\n"
     ]
    }
   ],
   "source": [
    "tokenized_text = multi_bert_tokenizer.tokenize(unk_text, add_special_tokens=False)\n",
    "print(tokenized_text)\n",
    "input_ids = multi_bert_tokenizer.encode(unk_text, add_special_tokens=False)\n",
    "print(input_ids)\n",
    "decoded_ids = multi_bert_tokenizer.decode(input_ids)\n",
    "print(decoded_ids)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5\n",
      "['한꾺인', '뜰만', '알', '##아', '##볼', '쑤', '있꼐', '짝썽하껬씁니따']\n",
      "[119547, 119548, 9524, 16985, 101450, 9510, 119550, 119551]\n",
      "한꾺인 뜰만 알아볼 쑤 있꼐 짝썽하껬씁니따\n"
     ]
    }
   ],
   "source": [
    "added_token_num = multi_bert_tokenizer.add_tokens(['한꾺인', '뜰만', '알아뽈', '있꼐', '짝썽하껬씁니따'])\n",
    "print(added_token_num)\n",
    "\n",
    "tokenized_text = multi_bert_tokenizer.tokenize(unk_text, add_special_tokens=False)\n",
    "print(tokenized_text)\n",
    "\n",
    "input_ids = multi_bert_tokenizer.encode(unk_text, add_special_tokens=False)\n",
    "print(input_ids)\n",
    "\n",
    "decoded_ids = multi_bert_tokenizer.decode(input_ids)\n",
    "print(decoded_ids)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- 특정역할을 수행하기 위해서 ,special token들도 추가할 수 있다.\n",
    "- 학습에 주로 사용되는 토큰들은 다음과 같다.\n",
    "- [BOS], [EOS], [UNK], [SEPT]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 불용어 처리"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "179\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\admin\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\admin\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "nltk.download('punkt')\n",
    "nltk.download('stopwords')\n",
    "\n",
    "from nltk.corpus import stopwords\n",
    "\n",
    "english_stopwords = stopwords.words('english')\n",
    "print(len(english_stopwords))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['back', 'translation', 'is', 'a', 'quality', 'assurance', 'technique', 'that', 'cad', 'add', 'clarity', 'to', 'and', 'control', 'over', 'your', 'translated', 'content', '!']\n",
      "['back', 'translation', 'quality', 'assurance', 'technique', 'cad', 'add', 'clarity', 'control', 'translated', 'content', '!']\n"
     ]
    }
   ],
   "source": [
    "from nltk.tokenize import word_tokenize\n",
    "\n",
    "eng_sample = 'back translation is a quality assurance technique that cad add clarity to and control over your translated content!'\n",
    "stop_words = set(stopwords.words('english'))\n",
    "\n",
    "tokens = word_tokenize(eng_sample)\n",
    "\n",
    "cleaned_tokens = []\n",
    "for w in tokens:\n",
    "    if w not in stop_words:\n",
    "        cleaned_tokens.append(w)\n",
    "        \n",
    "print(tokens)\n",
    "print(cleaned_tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# nltk 라이브러리의 불용어 사전은 한국어를 지원하지 않음\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "4560.55"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "4630 * 0.985"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "5093.0"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "4630 * 1.1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3826620.2934537246"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "3809422 * 1335 / 1329"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
